---
title: "Clustering"
date: "3/15/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(factoextra)
library(cluster)
library(ggthemes)
all_ages <- read.csv("data/all-ages.csv")
grad_students <- read.csv("data/grad-students.csv")
recent_grads <- read.csv("data/recent-grads.csv")
```

##Introduction

Given the large number of majors contained in this dataset, learning more about the economic benefits of specific majors, or major categories may be time-consuming. To make this process easier, we have created a tool that uses k-means clustering to group certain types of majors in broader categories based on unemployment rates and 25th, median, and 75th percentile income of a major's graduates. We chose an unsupervised learning method for this problem because we do not have a dataset that has majors grouped by the parameters we are interested in exploring.

Although the initial dataset contained a column that categorised each major into a larger major group, we found that these categories were not related to the earning potential of that major. So, we created our own labels to group majors based on unemployment rates and 25th, median, and 75th percentile income of a major's graduates. The data we used for this analysis contains information on income and employment rates for various majors for graduates of all ages, and not just recent grads.


##Methodology

To ensure we were using only our desired variables, we created a subset of our data using just the selected columns, and used this dataframe in determining our clusters. The first step was the standardize all the data we have to ensure minimum variance in distributions. Next, we determined the appropriate number of categories, or centers, our k-means analysis should use given the data we had only contained 173 observations. To do this, we used two methods:

```{r, include = FALSE}
#k-means for all_ages
majors_cluster <- all_ages %>% select(Unemployment_rate, P25th, P75th, Median)

majors_cluster <- scale(majors_cluster)

#Elbow method
wss <- (nrow(all_ages)-1)*sum(apply(majors_cluster,2,var))
wss
for (i in 2:15) wss[i] <- sum(kmeans(majors_cluster,
                                       centers=i)$withinss)

#kmeans clustering
set.seed(03152018)
majors_km <- kmeans(majors_cluster, centers = 5, nstart = 25)

all_ages <- all_ages %>%
  mutate(cluster = as.factor(majors_km$cluster))

majors_km
```

####The Elbow Method:
This method helps us determine the optimum number o clusters to create by calculating the sum of squared errors (SSE) for each intended value of k. Once the SSE's are plotted, we can visually interpret which values of k give us a lower SSE. The appropriate number of clusters to use is the point at which our distribution appears to have a "bend like an elbow joint", hence the name of method. However, the visual generated by our data does not appear to have a clear bending point, as seen below, making it hard to distinguish the appropriate number of centers for our analysis. Based on this graph, it may be worth exploring values of k between 4 and 6.

```{r, echo=FALSE}
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares", main = "Elbow Method Plot")
```

####Average Silhouette Method:
This method helps us determine the optimum number of clusters required to ensure objects lie neatly with their respective clusters, i.e. the highest quality of each cluster. According to this plot, the optimum number of centers for our analysis on this dataset is 2. However, 2 clusters does not provide students with nearly enough detail to filter the majors presented in our data.

```{r,echo=FALSE}
#Average Silhouette Method
fviz_nbclust(majors_cluster, kmeans, method = "silhouette")
```

Using the approximate distributions of the two plots above, and our own intuition of what is appropriate given our goals, we decided to try using 3-7 centers to cluster our data. Ultimately, our analysis demonstrated that 5 centers yielded the best results. With 5 centers, all majors in each cluster appeared to have a lot in common with one another in terms of field of study, and  professional and/or academic focus.

```{r, include=FALSE}
#Seeing majors within each cluster
all_ages %>%
  filter(cluster == 1) %>%
  select(Major_category, Major)

all_ages %>%
  filter(cluster == 2) %>%
  select(Major_category, Major)

all_ages %>%
  filter(cluster == 3) %>%
  select(Major_category, Major)

all_ages %>%
  filter(cluster == 4) %>%
  select(Major_category, Major)

all_ages %>%
  filter(cluster == 5) %>%
  select(Major_category, Major)
```

```{r, include = FALSE}
#Applying cluster labels
category = rep(NA, 173)
for (i in seq_along(all_ages$cluster)) {
  if (all_ages$cluster[i] == 1) {
    category[i] = "Education Majors/Vocational Majors"
  }
  if (all_ages$cluster[i] == 2) {
    category[i] = "Natural/Life/Physical Sciences"
  }
  if (all_ages$cluster[i] == 3) {
    category[i] = "Social Sciences & Arts"
  }
  if (all_ages$cluster[i] == 4) {
    category[i] = "Engineering"
  }
  if (all_ages$cluster[i] == 5) {
    category[i] = "IT/Math/Engineering"
  }
  category
}

all_ages[, "Major Category"] <- category
```

##Results

Once we had our clusters, we assigned each group an appropriate label. The five clusters are:


1. Education Majors and Vocational Majors
2. Natural/Life/Physical Sciences
3. Social Sciences and Arts
4. Engineering
5. IT/Math/Engineering


Given that our clusters also take into account unemployment rate, it was interesting to see that certain types of engineering were a part of their own group. It is also worth noting that many graduates with an education in a business fall in various clusters, depending on the business function they perform. Marketing majors and Operations & Supply Chain Management majors for instance, fall into completely different clusters, making it tricky to group business majors.

```{r, echo=FALSE}
ggplot(all_ages) + geom_point(aes(Median, Unemployment_rate, color = `Major Category`)) + 
  labs(x = "Median Income", 
       y = "Unemployment Rate", 
       title = "5 Major Clusters Using K-Means") +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  theme(legend.direction = 'vertical') +
  theme(legend.position = 'right')
```

Addtionally, when we attempted to perform a similar clustering analysis on our other datasets that contain information on only recent grads (>28 years of age) and on only graduate students (above 25 years of age), we found no discernable similiarities between the majors in each of the 5 resulting clusters. This analysis perhaps indicates that unemployment rates and income are not significantly impacted by major in most entry level jobs, but over longer periods of time. However, it is also worth noting that despite this, engineering majors did appear to dominate the clusters with higher salary ranges even in our analysis of these datasets.


```{r, include = FALSE}
#kmeans for recent grads

#Isolating variables and scaling
recent_grads_cluster <- recent_grads %>% select(Unemployment_rate, P25th, P75th, Median)

recent_grads_cluster <- scale(recent_grads_cluster)

#kmeans
set.seed(03152018)
recent_grads_km <- kmeans(recent_grads_cluster, centers = 5)

recent_grads <- recent_grads %>%
  mutate(cluster = as.factor(recent_grads_km$cluster))

recent_grads_km
```

```{r, include = FALSE}
#All majors within each cluster
recent_grads %>%
  filter(cluster == 1) %>%
  select(Major_category, Major)

recent_grads %>%
  filter(cluster == 2) %>%
  select(Major_category, Major)

recent_grads %>%
  filter(cluster == 3) %>%
  select(Major_category, Major)

recent_grads %>%
  filter(cluster == 4) %>%
  select(Major_category, Major)

recent_grads %>%
  filter(cluster == 5) %>%
  select(Major_category, Major)
```

```{r, include = FALSE}
#kmeans for grad students

#Isolating variables and scaling
grads_cluster <- grad_students %>% select(Grad_unemployment_rate, Grad_P25, Grad_P75, Grad_median)

grads_cluster <- scale(grads_cluster)

#Elbow method
wss <- (nrow(grad_students)-1)*sum(apply(grads_cluster,2,var))
wss
for (i in 2:15) wss[i] <- sum(kmeans(grads_cluster,
                                       centers=i)$withinss)

plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")

#Average Silhouette Method
fviz_nbclust(grads_cluster, kmeans, method = "silhouette")


#kmeans
set.seed(03152018)
grads_km <- kmeans(recent_grads_cluster, centers = 5)

grad_students <- grad_students %>%
  mutate(cluster = as.factor(grads_km$cluster))

grads_km
```

```{r, include = FALSE}
#Majors within each category
grad_students %>%
  filter(cluster == 1) %>%
  select(Major_category, Major)

grad_students %>%
  filter(cluster == 2) %>%
  select(Major_category, Major)

grad_students %>%
  filter(cluster == 3) %>%
  select(Major_category, Major)

grad_students %>%
  filter(cluster == 4) %>%
  select(Major_category, Major)

grad_students %>%
  filter(cluster == 5) %>%
  select(Major_category, Major)
```